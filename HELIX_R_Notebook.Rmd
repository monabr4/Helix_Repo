---
title: "Examining the Impact of Environmental Exposures on Body Mass Index (BMI)"
author: "Mona Bandov"
date: "Published: `r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_folding: hide
    theme: cosmo
    highlight: tango
  pdf_document:
    toc: true
    toc_depth: 2
    highlight: tango
    latex_engine: xelatex
    includes:
      in_header: preamble.tex
editor_options: 
  markdown: 
    wrap: 72
---

```{css, echo=FALSE}
body {
  font-family: 'Arial', sans-serif;
  line-height: 1.6;
  color: #333333;
}

h1, h2, h3, h4, h5, h6 {
  font-family: 'Georgia', serif;
  color: #2a3f54;
}

.section-box {
  border: 2px solid #cccccc;
  border-radius: 10px;
  padding: 15px;
  margin-bottom: 20px;
  background-color: #f9f9f9;
}

code {
  background-color: #f4f4f4;
  padding: 2px 4px;
  border-radius: 4px;
}

pre {
  max-height: 200px;
  overflow-y: auto;
}
```

```{r data setup, include=FALSE, echo=FALSE}

library(tidyverse)
library(dplyr)
library(ggplot2)
library(DT)
library(knitr)
library(kableExtra)
library(glmnet)
library(grpreg)
library(caret)
library(pROC)
library(randomForest)
library(rpart)
library(gbm)
library(nnet)
library(summarytools)
library(table1)
library(here)
library(corrplot)
library(rpart.plot)
library(fastshap)
library(car)
library(lmtest)
library(caret)
library(randomForest)
library(dplyr)
library(foreach)
library(doParallel)
library(Matrix)

dplyr::select

options(knitr.table.format = "html")
knitr::opts_chunk$set(echo = TRUE)
old.warn <- getOption("warn")
options(warn=-1)

work.dir <- here::here()
```

```{r}
#HELIX.RData Download: 
helix_file_path <- "~/Library/CloudStorage/GoogleDrive-bandov@usc.edu/.shortcut-targets-by-id/1oBvDKkpKxGnEoNogWDoXV--27W2spqKh/HELIX_data/HELIX.RData"
load(helix_file_path)
```

```{r}
# Metabol_serum.RData Download:
metabol_serum_file_path <- "~/Library/CloudStorage/GoogleDrive-bandov@usc.edu/.shortcut-targets-by-id/1oBvDKkpKxGnEoNogWDoXV--27W2spqKh/HELIX_data/metabol_serum.RData"
load(metabol_serum_file_path)
```

```{r}
#Adjusting BMI Category 
#Binning BMI so that predictive power increases 
phenotype <- phenotype %>%
  mutate(hs_bmi_c_cat = ifelse(hs_bmi_c_cat %in% c(1, 2), 0, 
                               ifelse(hs_bmi_c_cat %in% c(3, 4), 1, hs_bmi_c_cat)))
```


```{r}

#write.csv(full_data_with_meta_imputed_factored, "full_data_with_imputed_factored.csv", row.names = FALSE)
```


```{r, message=FALSE}
full_data_with_meta_imputed <- read_csv("full_data_with_meta_imputed.csv")
full_data_with_meta_imputed_factored<- read_csv("full_data_with_imputed_factored.csv")

```


# Abstract

This study examines the impact of prenatal environmental exposures on childhood body mass index (BMI) in children aged 6 to 11 years. Using data from the HELIX study, the influence of prenatal dietary intake, phthalate exposure concentrations, metabolomic urine data on BMI is investigated, while controlling for age, sex, and cohort. LASSO Regularization, Random Forest and Gradient Boosting Machines models were used to assess the predictive power of these prenatal exposures.

The LASSO performed better for the model with metabolomic data, followed by GBM and then random forest due to the nature of the data set.

# Hypothesis

How do prenatal dietary intake and concentrations of prenatal phthalates
exposures influence body mass index (BMI) for children aged 6 to 11
years, while controlling for child age, sex, and cohort? Additionally,
can urine metabolomics data improve predictive models of BMI using
statistical and machine learning tools?

# Introduction

![HELIX: Building the Early-Life Exposome](HELIX.png)



The relationship between prenatal environmental exposures and childhood
health outcomes is important, especially in understanding how prenatal
factors influence long-term health in children. This data project aims
to explore the influence of prenatal dietary intake and phthalate
exposure concentrations on body mass index (BMI) in children aged 6 to
11 years. Phthalates, commonly found in plastics and personal care
products, are known endocrine disruptors that may impact fetal
development and childhood growth patterns ([Valvi et al.,
2020](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8157593/)). Previous
studies have shown that prenatal exposure to phthalates is associated
with an increased risk of obesity and metabolic disorders in children
([Luo et al.,
2020](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7559247/)). By
controlling for key variables such as child age at examinations and
gestational age at birth, this project seeks to examine the direct and
interactive effects of these prenatal exposures on BMI.

Recent studies emphasize the important role of maternal diet during
pregnancy, demonstrating that balanced maternal nutrition can mitigate
the adverse effects of environmental exposures like phthalates on child
BMI, suggesting that improved dietary practices during pregnancy can
lead to better health outcomes for children ([NCBI,
2024](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9824240/)).

Phthalate exposure has been linked to various health issues, including
obesity, type II diabetes, thyroid dysfunction, higher blood pressure,
precocious puberty, and reproductive effects. It also impacts the
respiratory system (allergy, asthma) and nervous system (delayed
neurodevelopment, social impairment) ([Serrano et al.,
2021](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7483495/)).

Furthermore, this project looks at whether urine metabolomics data can
improve predictive models of BMI using statistical and machine learning
approaches. Integrating metabolomics data helps to understand the
biochemical pathways linking prenatal exposures (diet and phthalates) to
childhood health outcomes. This project aims to provide insights into
the prevention and management of childhood obesity and related health
conditions.

# Methods

## Data Preparation

### Imputation
Missing values were imputed with the mean for relevant columns. Rows were not removed to retain sample size.

### Data Splitting

A hold out set was implemented to train and test the data.

## Variables 

### Outcome Variable
- Body Mass Index (BMI) categorized (`hs_bmi_c_cat`)
  - 0: Underweight and Normal Weight 
  - 1: Overweight and Obese 

### Covariates
- `h_cohort`
- `e3_sex_None`
- `hs_child_age_None`

### Diet Variables (12 Diet)
- `e3_alcpreg_yn_None`
- `h_cereal_preg_Ter`
- `h_dairy_preg_Ter`
- `h_fastfood_preg_Ter`
- `h_folic_t1_None`
- `h_fruit_preg_Ter`
- `h_meat_preg_Ter`
- `h_pamod_t3_None`
- `h_fish_preg_Ter`
- `h_legume_preg_Ter`
- `h_pavig_t3_None`
- `h_veg_preg_Ter`

### Chemical Variables (10 Chemicals)
- `hs_mbzp_madj_Log2`
- `hs_mecpp_madj_Log2`
- `hs_mep_madj_Log2`
- `hs_mibp_madj_Log2`
- `hs_mnbp_madj_Log2`
- `hs_oxominp_madj_Log2`
- `hs_mehhp_madj_Log2`
- `hs_meohp_madj_Log2`
- `hs_ohminp_madj_Log2`
- `hs_mehp_madj_Log2`

### Meta Variables
Additional biochemical markers from urine metabolomics data. Click link below for more information:
<https://www.projecthelix.eu/files/HELIX_urine_metabol_DataSummary.pdf>\



---

## Statistical Analysis and Models 

<div style="border: 2px solid #cccccc; border-radius: 10px; padding: 15px; margin-bottom: 20px;">

### Primary Models

Model 1 (A-D) were constructed using Lasso regression techniques to see the effect of added predictors:
- **Lasso Regression**: Used to identify the most relevant predictors by applying L1 regularization.


### Model 1A: Outcome ~ Covariates

Examines the effect of covariates on BMI. 

### Model 1B: Outcome ~ Covariates + Diet

Examines the effect of covariates and diet on BMI. 

### Model 1C: Outcome ~ Covariates + Diet + Chemicals 

Examines the effect of covariates, diet, and chemical exposures on BMI. 

### Model 1D: Outcome ~ Covariates + Diet + Chemicals + Meta

Examines the effect of covariates, diet, chemical exposures, and metabolomic data on BMI.  



</div>

<div style="border: 2px solid #cccccc; border-radius: 10px; padding: 15px; margin-bottom: 20px;">

### Secondary Model (Used on the best model from Model 1)

- **Lasso Regression**: Used to identify the most relevant predictors by applying L1 regularization.
- **Random Forest**: Used to determine the best machine learning algorithm on the best performing dataset.
- **Gradient Boosting Machine (GBM)**: Used to determine the bet machine learning algorithm on the best performing dataset.

### Model 2A: LASSO on best performing model in Model 1

Applied LASSO to the best performing dataset with metabolmics data.

### Model 2B Random Forest best performing model in Model 1

Applied Random Forest (RF) to the best performing data set in Model 1. 


### Model 2C: Gradient Boosting Machine (GBM) on best performing model in Model 1

Applied to Gradient Boosting Machine (GBM) to the best performing data set in Model 1.


</div>

---

## Model Evaluation

<div style="border: 2px solid #cccccc; border-radius: 10px; padding: 15px; margin-bottom: 20px;">

### Performance Metrics

- **Area Under the Curve (AUC)**: ROC curve was used to assess the model's predictive accuracy.
- **ROC Curve Comparison**: ROC curves were plotted for both models to compare their performance visually.
- **Accuracy**: The proportion of true results (both true positives and true negatives) among the total number of cases examined.

</div>

# Data

The complexity of exposure to environmental contaminants has increased
because evolving environmental and lifestyle factors. The exposome
includes all environmental (non-genetic) exposures an individual
encounters from conception through old age. The HELIX (Human Early-Life
Exposome) project focuses on the early-life exposome, which integrates
all environmental hazards that mothers and children are exposed to and
linking these exposures to health, growth, and development risks.

Pregnancy and early childhood are critical times when children are more
vulnerable to environmental damage, which can have lifelong effects.
Understanding the exposome during these periods can help prevent
diseases, since early interventions can change biological foundation and
promote healthy development. HELIX cna help show how different
environmental exposures together affect health outcomes and risks.

There are six existing European birth cohort studies: Born in Bradford
(BiB), Etude des Déterminants pré et postnatals du développement et de
la santé de l'Enfant (EDEN), INfancia y Medio Ambiente (INMA), Kaunas
Cohort (KANC), Norwegian Mother, Father and Child Cohort Study (MoBa),
and Rhea Mother-Child Cohort Study. These cohorts have collected
extensive data from national and EU-funded projects. HELIX supplements
this data with advanced tools and methods to measure and integrate the
chemical, physical, and molecular environment, linking these
measurements to child health outcomes.



```{r image-helix, echo=FALSE, message=FALSE, fig.align='center', fig.cap='', out.width='100%', out.width='100%'}
image_file_path <- "~/Library/CloudStorage/GoogleDrive-bandov@usc.edu/.shortcut-targets-by-id/1oBvDKkpKxGnEoNogWDoXV--27W2spqKh/HELIX_data/HELIX.png"
knitr::include_graphics(image_file_path)
```


Smartphones are utilized to measure air pollution, UV radiation,
physical activity, and noise exposure. Advanced laboratory techniques
identify biological markers of various chemical exposures, such as
contaminants in food, consumer products, and water. HELIX has gathered
extensive exposome data from mothers and children, making it the largest
study on this topic. The study design is multilevel: the first level
includes 31,472 mother-child pairs recruited during pregnancy across the
six cohorts; the second level consists of a subcohort of 1301
mother-child pairs with detailed measurements of biomarkers, omics
signatures, and health outcomes at ages 6-11 years; and the third level
involves repeat-sampling panel studies with about 150 children and 150
pregnant women to collect personal exposure data.

This research focuses on a subcohort of 1301 mother-child pairs to
explore questions related to environmental exposures, omic data, and
their impact on health outcomes. Specifically, the project will examine
urine metabolomics data with Body Mass Index (BMI) as the primary
outcome of interest. The goal of this project is to provide a deeper
understanding of how early-life environmental exposures influence BMI,
potentially providing more information on how to apply early
intervention and disease prevention.

For more details on the study design see Vrijheid, Slama, et al. EHP
2014. see <https://www.projecthelix.eu/index.php/es/data-inventory> for
more information regarding the study.



# Codebook

```{r}
codebook_file_path <- "~/Library/CloudStorage/GoogleDrive-bandov@usc.edu/.shortcut-targets-by-id/1oBvDKkpKxGnEoNogWDoXV--27W2spqKh/HELIX_data/HELIX.RData"
load(codebook_file_path)
#  Chemicals
filtered_codebook_chemicals <- codebook %>%
  filter(domain == "Chemicals" & 
         family == "Phthalates" & 
         period == "Pregnancy" & 
         variable_name != "hs_sumDEHP_madj_Log2")

# Covariates 
filtered_codebook_covariates <- codebook %>%
  filter(domain == "Covariates" & 
         variable_name %in% c("e3_sex_None", "h_cohort", "hs_child_age_None"))

# Phenotype
filtered_codebook_phenotype <- codebook %>%
  filter(domain == "Phenotype" & 
         variable_name %in% c("hs_bmi_c_cat"))

# Lifestyle
filtered_codebook_lifestyles <- codebook %>%
  filter(domain == "Lifestyles" & period == "Pregnancy" & subfamily == "Diet")

# Combining all the information 
combined_codebook <- bind_rows(
  filtered_codebook_chemicals,
  filtered_codebook_covariates,
  filtered_codebook_phenotype,
  filtered_codebook_lifestyles
)


```

## Data Summary Exposures: Lifestyles (Diet)

```{r Lifestyles summary, attr.output='style="max-height: 100px;"',}
#Lifestyle 
filtered_codebook_lifestyles <- codebook %>%
  filter(domain == "Lifestyles" & period == "Pregnancy")
selectExposures <- filtered_codebook_lifestyles$variable_name
summarytools::view(dfSummary(exposome[,names(exposome) %in% selectExposures], 
                             style = 'grid', 
                             plain.ascii = FALSE, 
                             valid.col = FALSE, 
                             headings = FALSE), 
                   method = "render")
```

## Data Summary Exposures: Chemicals (Phthalates)

```{r Chemicals summary, attr.output='style="max-height: 100px;"',}
#Chemical
filtered_codebook_chemicals <- codebook %>%
  filter(domain == "Chemicals" & 
         family == "Phthalates" & 
         period == "Pregnancy" & 
         variable_name != "hs_sumDEHP_madj_Log2")
selectExposures <- filtered_codebook_chemicals$variable_name
summarytools::view(dfSummary(exposome[,names(exposome) %in% selectExposures], 
                             style = 'grid', 
                             plain.ascii = FALSE, 
                             valid.col = FALSE, 
                             headings = FALSE), 
                   method = "render")
```

## Data Summary Exposures: Covariate 

```{r}
# Covariates 
filtered_codebook_covariates <- codebook %>%
  filter(variable_name %in% c("e3_sex_None", "h_cohort", "hs_child_age_None"))
selectCovariates <- filtered_codebook_covariates$variable_name
summarytools::view(dfSummary(covariates[,names(covariates) %in% selectCovariates], 
                             style = 'grid', 
                             plain.ascii = FALSE, 
                             valid.col = FALSE, 
                             headings = FALSE), 
                   method = "render")
```

# Data Exploration

```{r, warning=FALSE, message=FALSE}
# Combining covariates with phenotype (Full Data)
# Lifestyle variables
filtered_codebook_lifestyles <- codebook %>%
  filter(domain == "Lifestyles" & period == "Pregnancy")
selectExposures_lifestyle <- filtered_codebook_lifestyles$variable_name

# Chemical variables
filtered_codebook_chemicals <- codebook %>%
  filter(domain == "Chemicals" & family == "Phthalates" & period == "Pregnancy" & variable_name != "hs_sumDEHP_madj_Log2")
selectExposures_chemicals <- filtered_codebook_chemicals$variable_name

# Covariate variables
filtered_codebook_covariates <- codebook %>%
  filter(variable_name %in% c("e3_sex_None", "h_cohort", "hs_child_age_None"))
selectCovariates <- filtered_codebook_covariates$variable_name

# Phenotype variables
filtered_codebook_phenotype <- codebook %>%
  filter(domain == "Phenotype" & variable_name %in% c("hs_bmi_c_cat"))
selectPhenotypes <- filtered_codebook_phenotype$variable_name
all_selected_variables <- c("ID", selectExposures_lifestyle, selectExposures_chemicals, selectCovariates, selectPhenotypes, "age")

# Subset the data
subset_exposome <- exposome %>% dplyr::select(all_of(selectExposures_lifestyle), all_of(selectExposures_chemicals))
subset_covariates <- covariates %>% dplyr::select(all_of(selectCovariates))
subset_phenotype <- phenotype %>% dplyr::select(all_of(selectPhenotypes))

# Final Merge
exposome_phenotype_covariates <- exposome %>%
  dplyr::select(ID, all_of(selectExposures_lifestyle), all_of(selectExposures_chemicals)) %>%
  left_join(covariates %>% dplyr::select(ID, all_of(selectCovariates)), by = "ID") %>%
  left_join(phenotype %>% dplyr::select(ID, all_of(selectPhenotypes)), by = "ID")

# Binning BMI
exposome_phenotype_covariates <- exposome_phenotype_covariates %>%
  mutate(hs_bmi_c_cat = ifelse(hs_bmi_c_cat %in% c(1, 2), 0, 
                               ifelse(hs_bmi_c_cat %in% c(3, 4), 1, hs_bmi_c_cat)))


```


```{r,warning=FALSE, message=FALSE}
#Creating Full and Important Data
#Initial Model (Model 1)

full_variables <- c("e3_alcpreg_yn_None", "h_cereal_preg_Ter", "h_dairy_preg_Ter", "h_fastfood_preg_Ter",
                    "h_fish_preg_Ter", "h_folic_t1_None", "h_fruit_preg_Ter", "h_legume_preg_Ter",
                    "h_meat_preg_Ter", "h_pamod_t3_None", "h_pavig_t3_None", "h_veg_preg_Ter",
                    "hs_mbzp_madj_Log2", "hs_mecpp_madj_Log2", "hs_mehhp_madj_Log2", "hs_mehp_madj_Log2",
                    "hs_meohp_madj_Log2", "hs_mep_madj_Log2", "hs_mibp_madj_Log2", "hs_mnbp_madj_Log2",
                    "hs_ohminp_madj_Log2", "hs_oxominp_madj_Log2", "e3_sex_None", "h_cohort",
                    "hs_child_age_None", "hs_bmi_c_cat")


# Variable extracted based on Model 1.

important_vars <- c("e3_alcpreg_yn_None", "h_cereal_preg_Ter", "h_dairy_preg_Ter", 
                    "h_fastfood_preg_Ter", "h_folic_t1_None", "h_fruit_preg_Ter", "h_meat_preg_Ter", 
                    "h_pamod_t3_None", "hs_mbzp_madj_Log2", "hs_mecpp_madj_Log2", "hs_mep_madj_Log2", 
                    "hs_mibp_madj_Log2", "hs_mnbp_madj_Log2", "hs_oxominp_madj_Log2", "h_cohort",
                    "e3_sex_None", "hs_child_age_None", "hs_bmi_c_cat")


full_dataset <- exposome_phenotype_covariates[full_variables]
important_dataset <- exposome_phenotype_covariates[important_vars]


full_dataset$ID <- seq_len(nrow(full_dataset))
important_dataset$ID <- seq_len(nrow(important_dataset))
```

```{r, warning=FALSE, message=FALSE}

#Merged Data + Meta:
transposed_data <- t(metabol_serum.d)
transposed_data <- as.data.frame(transposed_data)
transposed_data$ID <- rownames(transposed_data)
transposed_data <- transposed_data[, c(ncol(transposed_data), 1:(ncol(transposed_data)-1))]
rownames(transposed_data) <- NULL


full_data_with_meta <- merge(full_dataset,transposed_data , by = "ID", all = TRUE)
var_imp_data_with_meta <- merge(important_dataset, transposed_data, by = "ID", all = TRUE)


```

```{r, warning=FALSE, message=FALSE}

library(magrittr)
full_data_with_meta_imputed_factored <- full_data_with_meta %>%mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))


```


## Table by Cohort:

```{r, warning=FALSE, message=FALSE}
exposome_phenotype_covariates <- exposome_phenotype_covariates %>%
  mutate(
    e3_sex_None = factor(e3_sex_None),
    h_cohort = factor(h_cohort),
    hs_bmi_c_cat = factor(hs_bmi_c_cat)
  )

# Labels
label(exposome_phenotype_covariates$e3_sex_None) <- "Sex"
label(exposome_phenotype_covariates$h_cohort) <- "Cohort"
label(exposome_phenotype_covariates$hs_child_age_None) <- "Child's Age"
label(exposome_phenotype_covariates$e3_alcpreg_yn_None) <- "Alcohol During Pregnancy"
label(exposome_phenotype_covariates$h_cereal_preg_Ter) <- "Cereal Intake During Pregnancy"
label(exposome_phenotype_covariates$h_dairy_preg_Ter) <- "Dairy Intake During Pregnancy"
label(exposome_phenotype_covariates$h_fastfood_preg_Ter) <- "Fast Food Intake During Pregnancy"
label(exposome_phenotype_covariates$h_fish_preg_Ter) <- "Fish Intake During Pregnancy"
label(exposome_phenotype_covariates$h_folic_t1_None) <- "Folic Acid Intake"
label(exposome_phenotype_covariates$h_fruit_preg_Ter) <- "Fruit Intake During Pregnancy"
label(exposome_phenotype_covariates$h_legume_preg_Ter) <- "Legume Intake During Pregnancy"
label(exposome_phenotype_covariates$h_meat_preg_Ter) <- "Meat Intake During Pregnancy"
label(exposome_phenotype_covariates$h_pamod_t3_None) <- "Physical Activity (Moderate)"
label(exposome_phenotype_covariates$h_pavig_t3_None) <- "Physical Activity (Vigorous)"
label(exposome_phenotype_covariates$h_veg_preg_Ter) <- "Vegetable Intake During Pregnancy"
label(exposome_phenotype_covariates$hs_mbzp_madj_Log2) <- "MBzP (Log2)"
label(exposome_phenotype_covariates$hs_mecpp_madj_Log2) <- "MECPP (Log2)"
label(exposome_phenotype_covariates$hs_mehhp_madj_Log2) <- "MEHHP (Log2)"
label(exposome_phenotype_covariates$hs_mehp_madj_Log2) <- "MEHP (Log2)"
label(exposome_phenotype_covariates$hs_meohp_madj_Log2) <- "MEOHP (Log2)"
label(exposome_phenotype_covariates$hs_mep_madj_Log2) <- "MEP (Log2)"
label(exposome_phenotype_covariates$hs_mibp_madj_Log2) <- "MiBP (Log2)"
label(exposome_phenotype_covariates$hs_mnbp_madj_Log2) <- "MnBP (Log2)"
label(exposome_phenotype_covariates$hs_ohminp_madj_Log2) <- "OH-MiNP (Log2)"
label(exposome_phenotype_covariates$hs_oxominp_madj_Log2) <- "OXO-MiNP (Log2)"
label(exposome_phenotype_covariates$hs_bmi_c_cat) <- "BMI Category"

# Continuous variables
render_cont <- function(x) {
  sprintf("%.2f (%.2f)", mean(x, na.rm = TRUE), sd(x, na.rm = TRUE))
}

# Categorical variables
render_cat <- function(x) {
  paste0(names(table(x)), " (", table(x), ")", collapse = ", ")
}

# Make sure it's 27 columns
columns <- c("age", "hs_child_age_None", "e3_alcpreg_yn_None", "h_cereal_preg_Ter",
             "h_dairy_preg_Ter", "h_fastfood_preg_Ter", "h_fish_preg_Ter", "h_folic_t1_None",
             "h_fruit_preg_Ter", "h_legume_preg_Ter", "h_meat_preg_Ter", "h_pamod_t3_None",
             "h_pavig_t3_None", "h_veg_preg_Ter", "hs_mbzp_madj_Log2", "hs_mecpp_madj_Log2",
             "hs_mehhp_madj_Log2", "hs_mehp_madj_Log2", "hs_meohp_madj_Log2", "hs_mep_madj_Log2",
             "hs_mibp_madj_Log2", "hs_mnbp_madj_Log2", "hs_ohminp_madj_Log2", "hs_oxominp_madj_Log2", 
             "hs_bmi_c_cat", "e3_sex_None", "h_cohort")

# Stratified by cohort
table1(~  hs_child_age_None  + e3_alcpreg_yn_None + h_cereal_preg_Ter + h_dairy_preg_Ter +
         h_fastfood_preg_Ter + h_fish_preg_Ter + h_folic_t1_None + h_fruit_preg_Ter +
         h_legume_preg_Ter + h_meat_preg_Ter + h_pamod_t3_None + h_pavig_t3_None +
         h_veg_preg_Ter + hs_mbzp_madj_Log2 + hs_mecpp_madj_Log2 + hs_mehhp_madj_Log2 +
         hs_mehp_madj_Log2 + hs_meohp_madj_Log2 + hs_mep_madj_Log2 + hs_mibp_madj_Log2 +
         hs_mnbp_madj_Log2 + hs_ohminp_madj_Log2 + hs_oxominp_madj_Log2 | h_cohort,
       data = exposome_phenotype_covariates,
       render.continuous = render_cont, render.categorical = render_cat,
       overall = TRUE, topclass = "Rtable1-shade")


```

## Table by Sex:

```{r, warning=FALSE, message=FALSE}

# Stratified by sex
table1(~  hs_child_age_None  + e3_alcpreg_yn_None + h_cereal_preg_Ter + h_dairy_preg_Ter +
         h_fastfood_preg_Ter + h_fish_preg_Ter + h_folic_t1_None + h_fruit_preg_Ter +
         h_legume_preg_Ter + h_meat_preg_Ter + h_pamod_t3_None + h_pavig_t3_None +
         h_veg_preg_Ter + hs_mbzp_madj_Log2 + hs_mecpp_madj_Log2 + hs_mehhp_madj_Log2 +
         hs_mehp_madj_Log2 + hs_meohp_madj_Log2 + hs_mep_madj_Log2 + hs_mibp_madj_Log2 +
         hs_mnbp_madj_Log2 + hs_ohminp_madj_Log2 + hs_oxominp_madj_Log2 | e3_sex_None,
       data = exposome_phenotype_covariates,
       render.continuous = render_cont, render.categorical = render_cat,
       overall = TRUE, topclass = "Rtable1-shade")

```


## Data Summary Exposures of Outcome: Phenotype (BMI Category)

Since the original data contained 4 BMI categories, the distribution of those categories were not normal or representative of the population. Aggregation of the BMI variables were performed to ensure that the model is more reliable. 

```{r, warning=FALSE, message=FALSE}
summary_data <- full_dataset %>%
  count(hs_bmi_c_cat) %>%
  mutate(percentage = n / sum(n) * 100)

ggplot(data = summary_data, aes(x = as.factor(hs_bmi_c_cat), y = percentage)) +
  geom_bar(stat = "identity", fill = c("red", "blue")) +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            vjust = -0.5, size = 4) +
  labs(title = "Distribution of BMI Categories",
       x = "BMI Cateogories",
       y = "Percentage") +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))

```

After re-categorization, BMI category is moderately imbalanced. Given this moderate imbalance, we have decided to proceed with the model without making any adjustments. 


## Correlation Matrix

Below is a correlation matrix to help decide which model is most appropriate for the data.This matrix is used to assess presence of multicollinearity. 
```{r, warning=FALSE, message=FALSE}
numeric_vars <- exposome_phenotype_covariates %>%
  dplyr::select(where(is.numeric))

cor_matrix <- cor(numeric_vars, use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 60, tl.cex = 0.8)

```


The correlation analysis revealed the following highly correlated pairs (70% threshold):

```{r, message=FALSE, warning=FALSE}
find_highly_correlated <- function(cor_matrix, threshold = 0.8) {
  cor_matrix[lower.tri(cor_matrix, diag = TRUE)] <- NA  
  cor_matrix <- as.data.frame(as.table(cor_matrix)) 
  cor_matrix <- na.omit(cor_matrix)  
  cor_matrix <- cor_matrix[order(-abs(cor_matrix$Freq)), ]  
  cor_matrix <- cor_matrix %>% filter(abs(Freq) > threshold)  
  return(cor_matrix)
}
highly_correlated_pairs <- find_highly_correlated(cor_matrix, threshold = 0.70)
highly_correlated_pairs

```

Among these findings, `hs_mehhp_madj_Log2` was found to have a high correlation with `hs_meohp_madj_Log2`, with a correlation coefficient of 0.8076375. Given this high correlation, `hs_mehhp_madj_Log2` was removed from the model to address multicollinearity. 

Despite considering group LASSO logistic regression to handle multicollinearity, regular LASSO regression was used for model building. To address the issue of multicollinearity, the variable `hs_mehhp_madj_Log2` was removed from the model.



```{r, message=FALSE, warning=FALSE}
#Finalize the data set 
outcome <- "hs_bmi_c_cat"
covariates_final <- c("h_cohort", "e3_sex_None") # Did not include age

diet_final <- c("e3_alcpreg_yn_None", "h_cereal_preg_Ter", "h_dairy_preg_Ter", 
                "h_fastfood_preg_Ter", "h_folic_t1_None", "h_fruit_preg_Ter", 
                "h_meat_preg_Ter", "h_pamod_t3_None", "h_fish_preg_Ter", 
                "h_legume_preg_Ter", "h_pavig_t3_None", "h_veg_preg_Ter")
factorize_columns <- c(covariates_final, diet_final)
full_data_with_meta_imputed_factored <- full_data_with_meta_imputed %>%
  mutate(across(all_of(factorize_columns), as.factor))
```

# Model Building and Stastical Analysis 

### Primary Models: Model Building (1A,1B,1C,1D)

LASSO Regression model will be used to compare the performance of different models predicting BMI categories. LASSO regression, known for its ability to perform both variable selection and regularization, is used here to determine the impact of varying predictor sets on model accuracy. The goal is to identify which combination of predictor variables yields the best model performance. AUC and important variables will be highlighted.

* *Model 1A*: Outcome ~ Covariates 
* *Model 1B*: Outcomes ~ Covariates + Diet
* *Model 1C*: Outcomes ~ Covariates + Diet + Phthalates 
* *Model 1D*: Outcomes ~ Covariates + Diet + Phthalates + Metabolomic Data


#### ROC Curves for Model 1:

In this section, ROC (Receiver Operating Characteristic) curves will evaluate the performance of four different models. ROC curves are used to assess the ability of a classification model to distinguish between different BMI categories.


```{r, message=FALSE, warning=FALSE}


outcome <- "hs_bmi_c_cat"
covariates_final <- c("h_cohort", "e3_sex_None", "hs_child_age_None")

phthalates_final <- c("hs_mbzp_madj_Log2", "hs_mecpp_madj_Log2", "hs_mep_madj_Log2", 
                      "hs_mibp_madj_Log2", "hs_mnbp_madj_Log2", "hs_oxominp_madj_Log2", 
                      "hs_meohp_madj_Log2", "hs_ohminp_madj_Log2", 
                      "hs_mehp_madj_Log2") # removed the one variable hs_mehhp_madj_Log2

diet_final <- c("e3_alcpreg_yn_None", "h_cereal_preg_Ter", "h_dairy_preg_Ter", 
                "h_fastfood_preg_Ter", "h_folic_t1_None", "h_fruit_preg_Ter", 
                "h_meat_preg_Ter", "h_pamod_t3_None", "h_fish_preg_Ter", 
                "h_legume_preg_Ter", "h_pavig_t3_None", "h_veg_preg_Ter")

meta_final <- grep("^metab_", names(full_data_with_meta_imputed_factored), value = TRUE)

# predictor sets
predictor_sets <- list(
  covariate = covariates_final,
  covariates_diet = c(covariates_final, diet_final),
  covariates_diet_phthalates = c(covariates_final, diet_final, phthalates_final),
  covariates_diet_phthalates_meta = c(covariates_final, diet_final, phthalates_final, meta_final)
)

# Function 
model_performance <- function(predictors) {
  subset_data <- full_data_with_meta_imputed_factored[, c(predictors, outcome)]
  subset_data <- na.omit(subset_data)
  
  subset_data[[outcome]] <- as.factor(subset_data[[outcome]])
  
  # Hold out set
  set.seed(999)
  trainIndex <- createDataPartition(subset_data[[outcome]], p = .8, list = FALSE)
  train_data <- subset_data[trainIndex, ]
  test_data <- subset_data[-trainIndex, ]
  
  x_train <- model.matrix(as.formula(paste(outcome, "~ .")), data = train_data)[, -1]
  y_train <- train_data[[outcome]]
  x_test <- model.matrix(as.formula(paste(outcome, "~ .")), data = test_data)[, -1]
  y_test <- test_data[[outcome]]
  
  # Train
  covariates_train <- model.matrix(as.formula(paste(outcome, "~", paste(covariates_final, collapse = "+"))), data = train_data)[, -1]
  
  # Test
  covariates_test <- model.matrix(as.formula(paste(outcome, "~", paste(covariates_final, collapse = "+"))), data = test_data)[, -1]
  
  x_train <- cbind(covariates_train, x_train[, !colnames(x_train) %in% colnames(covariates_train)])
  x_test <- cbind(covariates_test, x_test[, !colnames(x_test) %in% colnames(covariates_test)])
  
  # Fit cv and alpha = 1 is LASSO
  cv_fit <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
  best_lambda <- cv_fit$lambda.min
  final_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = best_lambda)
  
  # Predict
  probabilities <- predict(final_model, s = best_lambda, newx = x_test, type = "response")
  
  roc_curve <- roc(y_test, as.numeric(probabilities))
  auc_value <- auc(roc_curve)
  coefficients <- coef(final_model, s = best_lambda)[-1, ]
  
  list(roc_curve = roc_curve, auc_value = auc_value, coefficients = coefficients)
}

# Look at all models
results <- lapply(predictor_sets, model_performance)

# ROC Curves
plot(results[[1]]$roc_curve, col = "blue", main = "ROC Curves for Model 1(A-D)", lwd = 2)
colors <- c("green", "red", "purple")
for (i in 2:length(results)) {
  lines(results[[i]]$roc_curve, col = colors[i-1], lwd = 2)
}


legend("bottomright", inset = c(-0.01, 0), legend = c(
  paste("Model 1A: Covariate (AUC =", round(results[[1]]$auc_value, 2), ")"), 
  paste("Model 1B: Covariates and Diet (AUC =", round(results[[2]]$auc_value, 2), ")"), 
  paste("Model 1C: Covariates, Diet, and Phthalates (AUC =", round(results[[3]]$auc_value, 2), ")"), 
  paste("Model 1D: Covariates, Diet, Phthalates, and Meta (AUC =", round(results[[4]]$auc_value, 2), ")")
), col = c("blue", "green", "red", "purple"), lwd = 1, cex = 0.7, xpd = TRUE)

```


The evaluation of model performance using ROC curves and AUC values showed a significant differences in predictive accuracy across the models. Model 1A, which only included covariates, had an AUC of 0.59, indicating limited discriminatory power. Adding dietary variables in Model 1B improved the AUC slightly to 0.61. Model 1C, which included both dietary variables and phthalates, showed a slight increase in AUC to 0.62. However, the most substantial improvement was observed in Model 1D, which added metabolomic data in addition to covariates, diet, and phthalates, achieved an AUC of 0.84. Adding metabolomic data significantly improved the predictive performance in distinguishing between the two BMI categories.



#### Variable Importance for Model 1:

Variable importance (Top 10) quantifies how much each predictor contributes to the performance of a predictive model. It helps identify which variables have the most significant impact on the model’s predictions. Here’s an overview of which variables are the most influential when building the model: 

```{r, message=FALSE, warning=FALSE}
par(mfrow = c(2, 2)) 
model_names <- c("Model 1A", "Model 1B", "Model 1C", "Model 1D")

for (i in 1:length(results)) {
  coeffs <- results[[i]]$coefficients
  top_coeffs <- coeffs[order(abs(coeffs), decreasing = TRUE)][1:10]
  colors <- ifelse(top_coeffs > 0, "blue", "red")
  barplot(top_coeffs, las = 2, main = paste("Variable Importance -", model_names[i]), cex.names = 0.48, col = colors)
}

```


The variable importance analysis across four models (1A, 1B, 1C, and 1D) shows how different variables are influential in predicting outcomes. In Model 1A, cohort variables such as `h_cohort3` and `h_cohort6` are the most influential, meaning that cohort has the biggest influence on the model's predictions. Moving to Model 1B, while cohort variables like `h_cohort5` and `h_cohort3` remain important, dietary factors including `h_meal_preg_Tc63` and `h_cereal_preg_Tc63` begin to show substantial influence, meaning that dietary habits during pregnancy significantly impact BMI on children age 6-11.

Model 1C presents a more complex interaction, where cohort variables continue to be influential, but dietary and phthalate variables such as `h_meal_preg_Tc63` and `e3_agepreg3y_None2` also important. The most infuential change is observed in Model 1D, where the addition of metabolomic data dramatically shifts the variable importance landscape. Metabolomic variables like `metab_161` and `metab_160` show the highest influence, far surpassing the importance of cohorts and other variables, highlighting the dominant predictive power of metabolomic data.

Overall, the analysis reveals that while cohort and dietary factors are significant, the inclusion of metabolomic data provides a much stronger predictive power. This means that metabolomic factors are important for understanding how BMI is affected. Moving forward, incorporating metabolomic data and exploring other machine learning algorithms will be help refine the predictions and help with a  deeper understanding of the effect of variables on BMI in children ages 6-11.

### Secondary Models: Advanced Machine Learning Modeling (2A,2B,2C)

In this section, the machine learning models will be refined to assess the best-performing model from Model 1. Based on the performance metrics, Model 1D, which includes metabolic data, showed the highest AUC result. LASSO, Random Forest, and GBM will be evaluated to see which model performs better when applied to the dataset from Model 1D. To  minimize the risk of overfitting, hyperparamters were tuned using the 10-fold cross-validation (CV=10) approach.


```{r, message=FALSE, warning=FALSE}
data <- full_data_with_meta_imputed_factored
outcome <- "hs_bmi_c_cat"
covariates_final <- c("h_cohort", "e3_sex_None")
phthalates_final <- c("hs_mbzp_madj_Log2", "hs_mecpp_madj_Log2", "hs_mep_madj_Log2", 
                      "hs_mibp_madj_Log2", "hs_mnbp_madj_Log2", "hs_oxominp_madj_Log2", 
                       "hs_meohp_madj_Log2", "hs_ohminp_madj_Log2", 
                      "hs_mehp_madj_Log2") 
diet_final <- c("e3_alcpreg_yn_None", "h_cereal_preg_Ter", "h_dairy_preg_Ter", 
                "h_fastfood_preg_Ter", "h_folic_t1_None", "h_fruit_preg_Ter", 
                "h_meat_preg_Ter", "h_pamod_t3_None", "h_fish_preg_Ter", 
                "h_legume_preg_Ter", "h_pavig_t3_None", "h_veg_preg_Ter")
meta_final <- grep("^metab_", names(data), value = TRUE)

all_vars <- c(covariates_final, phthalates_final, diet_final, meta_final)
X <- as.matrix(data[all_vars])
y <- data[[outcome]]  # outcome variable "hs_bmi_c_cat"

# Split training and testing sets
set.seed(999)
train_indices <- sample(1:nrow(X), size = 0.8 * nrow(X))
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
y_train <- y[train_indices]
y_test <- y[-train_indices]

# Fit the LASSO model using cross-validation
cv_fit <- cv.glmnet(X_train, y_train, alpha = 1, family = "binomial")

# Best lambda value
best_lambda <- cv_fit$lambda.min
#print(paste("Best lambda:", best_lambda))

# Fit the final model on the entire training data
lasso_model <- glmnet(X_train, y_train, alpha = 1, lambda = best_lambda, family = "binomial")

# Predict on the test data
y_pred_prob_lasso <- predict(lasso_model, s = best_lambda, newx = X_test, type = "response")
y_pred_lasso <- ifelse(y_pred_prob_lasso > 0.5, 1, 0)

# Calculate ROC and AUC for LASSO
roc_obj_lasso <- roc(y_test, y_pred_prob_lasso)
auc_value_lasso <- auc(roc_obj_lasso)
#print(paste("LASSO AUC:", auc_value_lasso))

# LASSO accuracy
accuracy_lasso <- mean(y_pred_lasso == y_test)
#print(paste("LASSO Accuracy:", accuracy_lasso))

data$hs_bmi_c_cat <- as.factor(data$hs_bmi_c_cat)

# HO set
set.seed(999) 
trainIndex <- createDataPartition(data$hs_bmi_c_cat, p = .8, list = FALSE, times = 1)
dataTrain <- data[trainIndex, ]
dataTest <- data[-trainIndex, ]

# training control
train_control <- trainControl(method = "cv", number = 10, search = "grid")

#  parameter grid for Random Forest
rf_tune_grid <- expand.grid(mtry = c(2, 4, 6, 8, 10))

# Train the Random Forest model with hp tuning
set.seed(999)
rf_model <- train(hs_bmi_c_cat ~ ., data = dataTrain, method = "rf", trControl = train_control, tuneGrid = rf_tune_grid)


#print(rf_model$bestTune)

# Predictions with Random Forest on the test set
rf_prob_predictions <- predict(rf_model, newdata = dataTest, type = "prob")[, 2]
rf_predictions <- predict(rf_model, newdata = dataTest)

# Evaluate the Random Forest model with confusion matrix
rf_conf_matrix <- confusionMatrix(rf_predictions, dataTest$hs_bmi_c_cat)
#print(rf_conf_matrix)

# ROC and AUC for Random Forest
rf_roc_curve <- roc(dataTest$hs_bmi_c_cat, rf_prob_predictions)
rf_auc_value <- auc(rf_roc_curve)
#print(paste("Random Forest AUC:", rf_auc_value))

# Random Forest accuracy
accuracy_rf <- rf_conf_matrix$overall['Accuracy']
#print(paste("Random Forest Accuracy:", accuracy_rf))

# GRID SEARCH
gbm_tune_grid <- expand.grid(interaction.depth = c(1, 3, 5), n.trees = c(50, 100, 150), shrinkage = c(0.01, 0.1), n.minobsinnode = 10)

# Train the GBM model with hyperparameter tuning
set.seed(999)
gbm_model <- train(hs_bmi_c_cat ~ ., data = dataTrain, method = "gbm", trControl = train_control, tuneGrid = gbm_tune_grid, verbose = FALSE)


#print(gbm_model$bestTune)

# Make predictions with GBM on the test set
gbm_prob_predictions <- predict(gbm_model, newdata = dataTest, type = "prob")[, 2]
gbm_predictions <- predict(gbm_model, newdata = dataTest)

# Evaluate the GBM model with confusion matrix
gbm_conf_matrix <- confusionMatrix(gbm_predictions, dataTest$hs_bmi_c_cat)
#print(gbm_conf_matrix)

# ROC and AUC for GBM
gbm_roc_curve <- roc(dataTest$hs_bmi_c_cat, gbm_prob_predictions)
gbm_auc_value <- auc(gbm_roc_curve)
#print(paste("GBM AUC:", gbm_auc_value))

# GBM accuracy
accuracy_gbm <- gbm_conf_matrix$overall['Accuracy']
#print(paste("GBM Accuracy:", accuracy_gbm))

# Combine ROC curves 
roc_data_lasso <- data.frame(
  tpr = roc_obj_lasso$sensitivities,
  fpr = 1 - roc_obj_lasso$specificities,
  model = paste("LASSO (AUC =", round(auc_value_lasso, 2), ")")
)

roc_data_rf <- data.frame(
  tpr = rf_roc_curve$sensitivities,
  fpr = 1 - rf_roc_curve$specificities,
  model = paste("Random Forest (AUC =", round(rf_auc_value, 2), ")")
)

roc_data_gbm <- data.frame(
  tpr = gbm_roc_curve$sensitivities,
  fpr = 1 - gbm_roc_curve$specificities,
  model = paste("GBM (AUC =", round(gbm_auc_value, 2), ")")
)

roc_data_combined <- rbind(roc_data_lasso, roc_data_rf, roc_data_gbm)

# ROC curves together
ggplot(roc_data_combined, aes(x = fpr, y = tpr, color = model)) +
  geom_line(size = 1) +
  geom_abline(linetype = "dashed") +
  labs(title = "ROC Curves for Model 2 (A-C)", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "green", "red"))

#cat("LASSO Accuracy:", accuracy_lasso, "\n")
#cat("Random Forest Accuracy:", accuracy_rf, "\n")
#cat("GBM Accuracy:", accuracy_gbm, "\n")

```


##### Summary of Model Training and Evaluation for Secondary Model

#### Data Preparation:

1. **Dataset**: The dataset used includes  covariates, phthalates, dietary variables, and metabolomic data.
2. **Outcome Variable**: The target variable for prediction is `hs_bmi_c_cat`.

#### Model Performance Summary

#### A. LASSO Regression
* **Best Lambda**: 0.00390940491989417
* **Performance**:
  * AUC: 0.8258
  * Accuracy: 80.46%
* **Variable Importance**: Located in Model 1D
* **Possible Reason for Best Performance**: 
  The LASSO model likely performed the best due to its shrinkage feature, which penalizes less important variables. This is particularly beneficial given the influence of the metabolomic data in the dataset, allowing LASSO to focus on the most predictive features.

#### B. Random Forest
* **Parameter Tuning**: The model was tuned using a grid search with the following best parameter:
  * `mtry`: 14
* **Performance**:
  * AUC: 0.7508
  * Accuracy: 72.97%
* **Variable Importance**:
  
```{r}
var_importance <- varImp(rf_model, scale = FALSE)
var_importance_df <- as.data.frame(var_importance$importance)
var_importance_df$Variable <- rownames(var_importance_df)
var_importance_df <- var_importance_df[order(-var_importance_df$Overall), ]
top_10_vars <- head(var_importance_df, 10)

ggplot(top_10_vars, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "red") +
  coord_flip() +
  labs(title = "Top 10 Variable Importance from Random Forest Model (Gini Importance)",
       x = "Variables",
       y = "Importance (Gini)") +
  theme_minimal()+theme(plot.title = element_text(size = 10))

```

This bar plot above displays the top 10 variables by importance in the Random Forest model, measured by Gini Importance. The most important variables are `metab_49`, `metab_95`, `metab_161`, and `metab_8`. Other key variables include `metab_30`, `hs_mnbp_madj_Log2`, `metab_163`, `metab_6`,`metab_48`, and `metab_96`. These variables significantly contribute to the model's predictive power, with `metab_49` having the highest impact.


#### C. Gradient Boosting Machine (GBM)
* **Parameter Tuning**: The model was tuned using a grid search with the following best parameters:
  * `n.trees`: 100
  * `interaction.depth`: 3
  * `shrinkage`: 0.1
  * `n.minobsinnode`: 10
* **Performance**:
  * AUC: 0.7757
  * Accuracy: 77.61%
* **Variable Importance**:

```{r, message=FALSE, warning=FALSE}

var_importance_gbm <- varImp(gbm_model, scale = FALSE)
var_importance_gbm_df <- as.data.frame(var_importance_gbm$importance)
var_importance_gbm_df$Variable <- rownames(var_importance_gbm_df)
var_importance_gbm_df <- var_importance_gbm_df[order(-var_importance_gbm_df$Overall), ]


top_10_vars_gbm <- head(var_importance_gbm_df, 10)
ggplot(top_10_vars_gbm, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +
  labs(title = "Top 10 Variable Importance from GBM Model",
       x = "Variables",
       y = "Importance") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10))  
```

This bar plot displays the top 10 variables by importance in the Gradient Boosting Machine (GBM) model. The most important variables are `metab_49`, `metab_161`, and `metab_95`. Other significant variables include `metab_8`, `metab_47`, `metab_30`, `metab_26`, `metab_120`, `metab_160`, and `metab_6`. These variables significantly contribute to the model's predictive power.

#### ROC and AUC Analysis:

* The ROC curves and AUC values for all three models were plotted to compare their performance visually. The LASSO model showed the highest AUC (0.8258), followed by GBM (0.7757), and Random Forest (0.7508).


#### Comparing Model Accuracy 2 (A-C)
```{r, message=FALSE, warning=FALSE}

models <- c("LASSO", "Random Forest", "GBM")
accuracy <- c(0.8045977, 0.7297297, 0.7760618)

data <- data.frame(models, accuracy)
data <- data[order(-data$accuracy),]
data$accuracy <- data$accuracy * 100

colors <- c("red", "green", "blue")

barplot(data$accuracy, 
        names.arg = data$models, 
        col = colors, 
        ylim = c(0, 100), 
        main = "Model Accuracy Comparison", 
        xlab = "Models", 
        ylab = "Accuracy (%)")

text(x = barplot(data$accuracy, plot = FALSE), 
     y = data$accuracy, 
     label = paste0(round(data$accuracy, 2), "%"), 
     pos = 3, 
     cex = 0.8, 
     col = "black")

```

This bar plot above compares the accuracy of three machine learning models: LASSO, GBM, and Random Forest on Model 1D. The LASSO model achieved the highest accuracy at 80.46%, followed by the GBM model at 77.61%, and the Random Forest model at 72.97%. This visualization highlights LASSO as the most accurate model among the three.


# Conclusion

In Model 2, LASSO, Random Forest, and GBM algorithms were evaluated on Model 1D. LASSO achieved the highest accuracy and AUC, demonstrating the best predictive performance. This is likely due to LASSO's shrinkage capability, which manages high-dimensional metabolomic data by focusing on relevant predictors.

Metabolomic data dominated variable importance in the LASSO (Model 1D), Random Forest, and GBM models. The strength of metabolomic data in these models may be attributed to the inherent biological relevance of these variables to BMI. Further analysis is needed to determine the optimal machine learning technique.

LASSO was chosen for this dataset for several reasons. Firstly, it achieved the highest performance in terms of both accuracy and AUC/ROC, indicating the best predictive capability. Additionally, LASSO's shrinkage factor is well-suited for handling high-dimensional data. LASSO effectively reduces the coefficients of less important variables to zero, focusing the model on the most relevant predictors and thereby mitigating overfitting.

The focus is to ensure that all relevant variables are being considered in the model while maximizing predictive performance. Therefore, LASSO is preferred for this dataset due to its ability to handle large and complex datasets, combined with its strong performance metrics.

# Future Work and Limitations

*Assess Different Models*: While LASSO has shown the best performance, further evaluation of other machine learning models could provide additional information and improvements.

*Assess Multicollinearity Among Metabolite Data*: Investigating the multicollinearity among metabolite variables will help to understand their interrelationships and to make sure that the model's assumptions are not violated.

*Model Diagnostics*: Perform residual analysis to check the assumptions of the regression model, such as linearity, independence, homoscedasticity, and normality of residuals.

*Hyperparameter Tuning*: Optimize the LASSO regularization parameter (lambda) through cross-validation (ex. LOOCV) to ensure the model is neither over-penalizing nor under-penalizing the predictors.

By undertaking these additional consideration, informed decision-making can contribute to better predictive performance.

# Reference
- <https://cran.r-project.org/web/packages/grplasso/grplasso.pdf>
- [Valvi et al., 2020](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8157593/)
- [Luo et al., 2020](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7559247/)
- [NCBI, 2024](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9824240/)
- [Serrano et al., 2021](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7483495/)



